---
title: "What Happens in Vegas: Predicting Hotel Ratings From Hotel and User Data"
author: "Bronwyn Baillie, Arun Maria, Manish Joshi </br>"
date: "2020/1/24 (updated: `r Sys.Date()`)"
always_allow_html: true
output: 
  github_document:
    toc: true
bibliography: vegas_hotels_refs.bib 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(knitr)
library(kableExtra)
library(tidyverse)
library(caret)
```


# Summary

In this project, we build a regression model that uses data collected from TripAdvisor user reviews of Las Vegas hotels to predict what kind of user ratings can be expected for a hotel [@vegas].  Exploratory data analysis was performed in R on only a portion of our data reserved for training [@R]. In our preliminary EDA, we checked the effect of different user and hotel specific features against user ratings, and it became apparent that most of the user specific features did not have much impact on the hotel ratings. However, upon further analysis throughout our project we discovered that some features, such as the presence of a swimming pool, free wifi, and user continent, did have an effect on hotel ratings.


# Introduction

Most travel and hotel bookings nowadays are being made online, and one of the key parameters a potential consumer refers to before deciding on which hotel to book is the ratings given to a hotel by users who had previously visited. For this project we wanted to be able to predict the expected user rating of a hotel, given various features about both the user and the hotel, and determine which user and hotel features had the most effect on that rating. Tourism industry professionals, travel agents, investors, and Hotel owners who wish to attract clients can draw benefit from such a model. 

# Methods

## Data

The Dataset chosen for the research project is the “Las Vegas Strip Dataset” which is collated information about customer feedback on 21 Hotels located in the Las Vegas Strip. The data is extracted from popular, respected and well-regarded travel portal “TripAdvisor”. 
-Moro, S., Rita, P., & Coelho, J. (2017). Stripping customers' feedback on hotels through data mining: The case of Las Vegas Strip. Tourism Management Perspectives, 23, 41-52. 
It was sourced from the UCI machine learning repositories, and can be found [here](https://archive.ics.uci.edu/ml/datasets/Las+Vegas+Strip). Each row in the data set represents information about one user review, and contains variables such as the name of the hotel, amenities present in the hotel such as a swimming pool, spa, or wifi, imformation about the reviewer like the number of reviews they've given and the number of years they've been a member, all along with the user's rating for the hotel.


## Analysis

From the preliminary EDA, it became apparent that most of the user specific features did not have much impact on the hotel ratings, whereas other features such as the presence of a swimming pool and free wifi showed a potential effect. Many of these realizations came from visualizations made using the ggplot2 library in R [@ggplot2]. 

```{r echo=FALSE, fig.cap="Figure 1. Hotel scores distribution for numeric features", out.width = '100%'}
knitr::include_graphics("../src/eda_plots/numeric_predictor_distributions_across_scores.png")

```
```{r echo=FALSE, fig.cap="Figure 2. Average Hotel scores based on  categorical features", out.width = '100%'}
knitr::include_graphics("../src/eda_plots/score_distributions_across_predictors.png")

```


The training of the model was done in Python [@Python], and mean squared error (MSE) was used to measure model performance. We chose to use MSE because our goal is to create a model that has very strong predictive abilities, and MSE is a good way of interpreting how well a model performs. The model performed poorly with a high MSE when all the features were used in the training dataset. We then tried a Linear regression model and used scikit-learn's recursive feature elimination function to remove unnecessary features and improve our model [@scikit-learn]. The improvements in error can be seen visualized below using altair [@2018-altair].

```{r echo=FALSE, fig.cap="Figure 3. Realation between number of features and validation error", out.width = '70%'}
knitr::include_graphics("../results/result_features.png")

```


The model showed the most promise in terms of both training and validation errors when we limited our model to have just 3 features. The 3 features selected were whether or not the hotel had a swimming pool, whether or not the hotel had free Wifi, and the user's continent. These metrics were in line with what was seen during the EDA stage.



The Model was then trained using different regression algorithms such as a linear regression, an RFregressor, Lasso, ridge regression, and SVM, all from the sklearn package in Python. 10-fold cross validation was used to determine the best hyperparameters. The detailed results of hyperparameter optimization can be found [here](../results/hyperparamter_results.csv). It was found that the best performing algorithm on the validation set was the ridge regression (regularized linear regression) and this was chosen as the final model.


# Results

- Results of the model while hyperparameters were being tuned are shown below. The error metric used is MSE (mean square error).

We chose the value of the hyperparameter alpha based on the minimum value of the MSE.



```{r echo=FALSE, fig.cap="Figure 4. The error metric used is Mean Square Error (MSE).", out.width = '70%'}
knitr::include_graphics("../results/cv_results.png")
```

- The final error obtained on the unseen test data is 0.867 which is similar to the training error obtained. The model generalizes well and does not overfit. 

- The model is not overfit but based on the value of the final error it is apparent that the model is not performing upto the standards that were aspired for.

# Discussion

Considering the results of our analysis, we can see there is some potential for improving the model as a MSE of 0.867 is not that good considering ratings of hotels vary from 1 to 5.

Also the dataset is unbalanced(more ratings in 3 and 4 range), so in retrospect it may have been  beneficial to attempt to build a classification  model instead of the regression model. If this analysis is revisited in the future we may include such a model.


# References